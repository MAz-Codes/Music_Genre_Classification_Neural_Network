{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 1690)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               865792    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1014218 (3.87 MB)\n",
      "Trainable params: 1014218 (3.87 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 6.6614 - accuracy: 0.3487 - val_loss: 3.4748 - val_accuracy: 0.3827\n",
      "Epoch 2/50\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 2.5036 - accuracy: 0.4510 - val_loss: 2.7478 - val_accuracy: 0.4116\n",
      "Epoch 3/50\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 1.7514 - accuracy: 0.5223 - val_loss: 2.3116 - val_accuracy: 0.4617\n",
      "Epoch 4/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 1.3501 - accuracy: 0.5968 - val_loss: 2.2870 - val_accuracy: 0.4702\n",
      "Epoch 5/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 1.1220 - accuracy: 0.6503 - val_loss: 2.0850 - val_accuracy: 0.4831\n",
      "Epoch 6/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.9388 - accuracy: 0.6941 - val_loss: 2.0796 - val_accuracy: 0.5135\n",
      "Epoch 7/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.8504 - accuracy: 0.7267 - val_loss: 2.1296 - val_accuracy: 0.5017\n",
      "Epoch 8/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.7165 - accuracy: 0.7626 - val_loss: 2.0268 - val_accuracy: 0.5117\n",
      "Epoch 9/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.6121 - accuracy: 0.7961 - val_loss: 2.0719 - val_accuracy: 0.5243\n",
      "Epoch 10/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.5527 - accuracy: 0.8093 - val_loss: 2.1063 - val_accuracy: 0.5165\n",
      "Epoch 11/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4636 - accuracy: 0.8429 - val_loss: 2.0325 - val_accuracy: 0.5272\n",
      "Epoch 12/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4092 - accuracy: 0.8655 - val_loss: 2.0508 - val_accuracy: 0.5521\n",
      "Epoch 13/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.3809 - accuracy: 0.8663 - val_loss: 2.0559 - val_accuracy: 0.5350\n",
      "Epoch 14/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.3103 - accuracy: 0.8957 - val_loss: 2.0175 - val_accuracy: 0.5509\n",
      "Epoch 15/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.2755 - accuracy: 0.9066 - val_loss: 2.0799 - val_accuracy: 0.5587\n",
      "Epoch 16/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.3007 - accuracy: 0.9006 - val_loss: 2.1027 - val_accuracy: 0.5635\n",
      "Epoch 17/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.2529 - accuracy: 0.9138 - val_loss: 2.1236 - val_accuracy: 0.5617\n",
      "Epoch 18/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.2107 - accuracy: 0.9298 - val_loss: 2.1422 - val_accuracy: 0.5576\n",
      "Epoch 19/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1968 - accuracy: 0.9352 - val_loss: 2.2150 - val_accuracy: 0.5643\n",
      "Epoch 20/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1892 - accuracy: 0.9390 - val_loss: 2.0589 - val_accuracy: 0.5765\n",
      "Epoch 21/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1747 - accuracy: 0.9447 - val_loss: 2.1494 - val_accuracy: 0.5702\n",
      "Epoch 22/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1556 - accuracy: 0.9508 - val_loss: 2.2152 - val_accuracy: 0.5702\n",
      "Epoch 23/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1395 - accuracy: 0.9576 - val_loss: 2.1794 - val_accuracy: 0.5776\n",
      "Epoch 24/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.1379 - accuracy: 0.9573 - val_loss: 2.3544 - val_accuracy: 0.5691\n",
      "Epoch 25/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.0853 - accuracy: 0.9776 - val_loss: 2.3031 - val_accuracy: 0.5735\n",
      "Epoch 26/50\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.1082 - accuracy: 0.9667 - val_loss: 2.2519 - val_accuracy: 0.5721\n",
      "Epoch 27/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.3120 - accuracy: 0.8982 - val_loss: 2.3438 - val_accuracy: 0.5517\n",
      "Epoch 28/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.2354 - accuracy: 0.9211 - val_loss: 2.3677 - val_accuracy: 0.5795\n",
      "Epoch 29/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1165 - accuracy: 0.9646 - val_loss: 2.2382 - val_accuracy: 0.5910\n",
      "Epoch 30/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0835 - accuracy: 0.9744 - val_loss: 2.1919 - val_accuracy: 0.5850\n",
      "Epoch 31/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0504 - accuracy: 0.9879 - val_loss: 2.2458 - val_accuracy: 0.6002\n",
      "Epoch 32/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.0315 - accuracy: 0.9940 - val_loss: 2.2242 - val_accuracy: 0.6024\n",
      "Epoch 33/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0172 - accuracy: 0.9979 - val_loss: 2.1946 - val_accuracy: 0.6099\n",
      "Epoch 34/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0204 - accuracy: 0.9963 - val_loss: 2.2255 - val_accuracy: 0.6136\n",
      "Epoch 35/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1322 - accuracy: 0.9608 - val_loss: 2.8682 - val_accuracy: 0.5506\n",
      "Epoch 36/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.5492 - accuracy: 0.8414 - val_loss: 2.6064 - val_accuracy: 0.5632\n",
      "Epoch 37/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1731 - accuracy: 0.9447 - val_loss: 2.3644 - val_accuracy: 0.5817\n",
      "Epoch 38/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1077 - accuracy: 0.9638 - val_loss: 2.2307 - val_accuracy: 0.6043\n",
      "Epoch 39/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0518 - accuracy: 0.9878 - val_loss: 2.4871 - val_accuracy: 0.5958\n",
      "Epoch 40/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.0827 - accuracy: 0.9738 - val_loss: 2.2880 - val_accuracy: 0.6150\n",
      "Epoch 41/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0431 - accuracy: 0.9879 - val_loss: 2.5916 - val_accuracy: 0.5895\n",
      "Epoch 42/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0555 - accuracy: 0.9835 - val_loss: 2.4388 - val_accuracy: 0.5947\n",
      "Epoch 43/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0506 - accuracy: 0.9851 - val_loss: 2.6557 - val_accuracy: 0.5802\n",
      "Epoch 44/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1185 - accuracy: 0.9617 - val_loss: 2.6466 - val_accuracy: 0.5835\n",
      "Epoch 45/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1219 - accuracy: 0.9590 - val_loss: 2.7408 - val_accuracy: 0.5954\n",
      "Epoch 46/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0492 - accuracy: 0.9844 - val_loss: 2.6544 - val_accuracy: 0.6058\n",
      "Epoch 47/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0192 - accuracy: 0.9962 - val_loss: 2.3540 - val_accuracy: 0.6147\n",
      "Epoch 48/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.1503 - accuracy: 0.9547 - val_loss: 2.7714 - val_accuracy: 0.5869\n",
      "Epoch 49/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1196 - accuracy: 0.9595 - val_loss: 2.6069 - val_accuracy: 0.5917\n",
      "Epoch 50/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1067 - accuracy: 0.9636 - val_loss: 2.5017 - val_accuracy: 0.6036\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "# Path to the dataset file\n",
    "DATASET_PATH = \"data.json\"\n",
    "\n",
    "def load_data(dataset_path):\n",
    "    \"\"\"\n",
    "    Loads the dataset from a JSON file and returns the inputs and targets as numpy arrays.\n",
    "\n",
    "    Parameters:\n",
    "    dataset_path (str): Path to the dataset file.\n",
    "\n",
    "    Returns:\n",
    "    inputs (np.array): The MFCCs extracted from the audio files.\n",
    "    targets (np.array): The corresponding labels (genres) for the audio files.\n",
    "    \"\"\"\n",
    "    with open(dataset_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    inputs = np.array(data[\"mfcc\"])\n",
    "    targets = np.array(data[\"labels\"])\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset\n",
    "    inputs, targets = load_data(DATASET_PATH)\n",
    "\n",
    "    # Split data into training and testing sets (70% training, 30% testing)\n",
    "    inputs_train, inputs_test, targets_train, targets_test = \\\n",
    "        train_test_split(inputs, targets, test_size=0.3)\n",
    "\n",
    "    # Build the neural network architecture\n",
    "    model = keras.Sequential([\n",
    "        # Input layer: Flatten the input to be fed into the dense layers\n",
    "        keras.layers.Flatten(input_shape=(inputs.shape[1], inputs.shape[2])),\n",
    "        # 1st hidden layer with 512 neurons and ReLU activation function\n",
    "        keras.layers.Dense(512, activation=\"relu\"),\n",
    "        # 2nd hidden layer with 256 neurons and ReLU activation function\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        # 3rd hidden layer with 64 neurons and ReLU activation function\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        # Output layer with 10 neurons (one for each genre) using softmax activation\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    # Compile the model using Adam optimizer and a learning rate of 0.0001\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\", # Suitable loss function for classification\n",
    "        metrics=[\"accuracy\"] # Track accuracy during training\n",
    "    )\n",
    "\n",
    "    # Print the summary of the model architecture\n",
    "    model.summary()\n",
    "\n",
    "    # Train the model using the training data\n",
    "    model.fit(\n",
    "        inputs_train,\n",
    "        targets_train,\n",
    "        validation_data=(inputs_test, targets_test), # Validate on the test set\n",
    "        epochs=50, # Train for 50 epochs\n",
    "        batch_size=32 # Use a batch size of 32\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
