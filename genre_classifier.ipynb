{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 1690)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               865792    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1014218 (3.87 MB)\n",
      "Trainable params: 1014218 (3.87 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 6.3472 - accuracy: 0.3578 - val_loss: 3.5081 - val_accuracy: 0.4031\n",
      "Epoch 2/50\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 2.4089 - accuracy: 0.4535 - val_loss: 2.4312 - val_accuracy: 0.4398\n",
      "Epoch 3/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 1.6986 - accuracy: 0.5239 - val_loss: 2.0572 - val_accuracy: 0.4465\n",
      "Epoch 4/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 1.3506 - accuracy: 0.5809 - val_loss: 1.9513 - val_accuracy: 0.4820\n",
      "Epoch 5/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 1.1037 - accuracy: 0.6363 - val_loss: 1.9030 - val_accuracy: 0.5169\n",
      "Epoch 6/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.9419 - accuracy: 0.6840 - val_loss: 1.8237 - val_accuracy: 0.5109\n",
      "Epoch 7/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.8020 - accuracy: 0.7243 - val_loss: 1.8184 - val_accuracy: 0.5365\n",
      "Epoch 8/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.6848 - accuracy: 0.7623 - val_loss: 1.7617 - val_accuracy: 0.5272\n",
      "Epoch 9/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.5934 - accuracy: 0.7942 - val_loss: 1.7067 - val_accuracy: 0.5598\n",
      "Epoch 10/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.5468 - accuracy: 0.8147 - val_loss: 1.7804 - val_accuracy: 0.5402\n",
      "Epoch 11/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4569 - accuracy: 0.8429 - val_loss: 1.7638 - val_accuracy: 0.5487\n",
      "Epoch 12/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.3862 - accuracy: 0.8641 - val_loss: 1.7836 - val_accuracy: 0.5654\n",
      "Epoch 13/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.3453 - accuracy: 0.8790 - val_loss: 1.8142 - val_accuracy: 0.5576\n",
      "Epoch 14/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.3193 - accuracy: 0.8914 - val_loss: 1.8559 - val_accuracy: 0.5672\n",
      "Epoch 15/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.3023 - accuracy: 0.9006 - val_loss: 1.7772 - val_accuracy: 0.5647\n",
      "Epoch 16/50\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.2916 - accuracy: 0.9036 - val_loss: 1.8286 - val_accuracy: 0.5676\n",
      "Epoch 17/50\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.2217 - accuracy: 0.9263 - val_loss: 1.9222 - val_accuracy: 0.5639\n",
      "Epoch 18/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.2210 - accuracy: 0.9269 - val_loss: 1.9988 - val_accuracy: 0.5635\n",
      "Epoch 19/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1881 - accuracy: 0.9403 - val_loss: 1.9378 - val_accuracy: 0.5787\n",
      "Epoch 20/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1406 - accuracy: 0.9592 - val_loss: 1.8932 - val_accuracy: 0.5880\n",
      "Epoch 21/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.2323 - accuracy: 0.9223 - val_loss: 2.0149 - val_accuracy: 0.5791\n",
      "Epoch 22/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1643 - accuracy: 0.9468 - val_loss: 1.9805 - val_accuracy: 0.5939\n",
      "Epoch 23/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1954 - accuracy: 0.9323 - val_loss: 2.1267 - val_accuracy: 0.5802\n",
      "Epoch 24/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1415 - accuracy: 0.9525 - val_loss: 1.9785 - val_accuracy: 0.5943\n",
      "Epoch 25/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.1217 - accuracy: 0.9624 - val_loss: 2.0333 - val_accuracy: 0.5924\n",
      "Epoch 26/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.1511 - accuracy: 0.9506 - val_loss: 2.1769 - val_accuracy: 0.5828\n",
      "Epoch 27/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.1459 - accuracy: 0.9511 - val_loss: 2.1542 - val_accuracy: 0.5924\n",
      "Epoch 28/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.0891 - accuracy: 0.9727 - val_loss: 2.0551 - val_accuracy: 0.6032\n",
      "Epoch 29/50\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 0.0688 - accuracy: 0.9795 - val_loss: 2.0416 - val_accuracy: 0.6069\n",
      "Epoch 30/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0648 - accuracy: 0.9803 - val_loss: 2.0354 - val_accuracy: 0.6165\n",
      "Epoch 31/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1901 - accuracy: 0.9401 - val_loss: 2.3336 - val_accuracy: 0.5565\n",
      "Epoch 32/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.2322 - accuracy: 0.9198 - val_loss: 2.3703 - val_accuracy: 0.5821\n",
      "Epoch 33/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.1976 - accuracy: 0.9333 - val_loss: 2.2695 - val_accuracy: 0.5991\n",
      "Epoch 34/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0833 - accuracy: 0.9754 - val_loss: 2.0948 - val_accuracy: 0.6180\n",
      "Epoch 35/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0528 - accuracy: 0.9844 - val_loss: 2.2689 - val_accuracy: 0.6117\n",
      "Epoch 36/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0316 - accuracy: 0.9929 - val_loss: 2.1479 - val_accuracy: 0.6317\n",
      "Epoch 37/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0211 - accuracy: 0.9956 - val_loss: 2.2146 - val_accuracy: 0.6336\n",
      "Epoch 38/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.0877 - accuracy: 0.9732 - val_loss: 2.2661 - val_accuracy: 0.6058\n",
      "Epoch 39/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.3824 - accuracy: 0.8825 - val_loss: 2.2675 - val_accuracy: 0.5991\n",
      "Epoch 40/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.1575 - accuracy: 0.9479 - val_loss: 2.1079 - val_accuracy: 0.6236\n",
      "Epoch 41/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.0321 - accuracy: 0.9929 - val_loss: 2.2358 - val_accuracy: 0.6139\n",
      "Epoch 42/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0198 - accuracy: 0.9968 - val_loss: 2.1912 - val_accuracy: 0.6336\n",
      "Epoch 43/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0082 - accuracy: 0.9990 - val_loss: 2.1454 - val_accuracy: 0.6321\n",
      "Epoch 44/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0067 - accuracy: 0.9994 - val_loss: 2.2122 - val_accuracy: 0.6391\n",
      "Epoch 45/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.0096 - accuracy: 0.9984 - val_loss: 2.2485 - val_accuracy: 0.6350\n",
      "Epoch 46/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.0076 - accuracy: 0.9990 - val_loss: 2.2728 - val_accuracy: 0.6254\n",
      "Epoch 47/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.0078 - accuracy: 0.9989 - val_loss: 2.3232 - val_accuracy: 0.6273\n",
      "Epoch 48/50\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4111 - accuracy: 0.8834 - val_loss: 2.7918 - val_accuracy: 0.5169\n",
      "Epoch 49/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.3072 - accuracy: 0.9022 - val_loss: 2.4582 - val_accuracy: 0.5873\n",
      "Epoch 50/50\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1113 - accuracy: 0.9635 - val_loss: 2.1880 - val_accuracy: 0.6228\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "# Path to the dataset file\n",
    "DATASET_PATH = \"data.json\"\n",
    "\n",
    "def load_data(dataset_path):\n",
    "    \"\"\"\n",
    "    Loads the dataset from a JSON file and returns the inputs and targets as numpy arrays.\n",
    "\n",
    "    Parameters:\n",
    "    dataset_path (str): Path to the dataset file.\n",
    "\n",
    "    Returns:\n",
    "    inputs (np.array): The MFCCs extracted from the audio files.\n",
    "    targets (np.array): The corresponding labels (genres) for the audio files.\n",
    "    \"\"\"\n",
    "    with open(dataset_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    inputs = np.array(data[\"mfcc\"])\n",
    "    targets = np.array(data[\"labels\"])\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset\n",
    "    inputs, targets = load_data(DATASET_PATH)\n",
    "\n",
    "    # Split data into training and testing sets (70% training, 30% testing)\n",
    "    inputs_train, inputs_test, targets_train, targets_test = \\\n",
    "        train_test_split(inputs, targets, test_size=0.3)\n",
    "\n",
    "    # Build the neural network architecture\n",
    "    model = keras.Sequential([\n",
    "        # Input layer: Flatten the input to be fed into the dense layers\n",
    "        keras.layers.Flatten(input_shape=(inputs.shape[1], inputs.shape[2])),\n",
    "        # 1st hidden layer with 512 neurons and ReLU activation function\n",
    "        keras.layers.Dense(512, activation=\"relu\"),\n",
    "        # 2nd hidden layer with 256 neurons and ReLU activation function\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        # 3rd hidden layer with 64 neurons and ReLU activation function\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        # Output layer with 10 neurons (one for each genre) using softmax activation\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    # Compile the model using Adam optimizer and a learning rate of 0.0001\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\", # Suitable loss function for classification\n",
    "        metrics=[\"accuracy\"] # Track accuracy during training\n",
    "    )\n",
    "\n",
    "    # Print the summary of the model architecture\n",
    "    model.summary()\n",
    "\n",
    "    # Train the model using the training data\n",
    "    model.fit(\n",
    "        inputs_train,\n",
    "        targets_train,\n",
    "        validation_data=(inputs_test, targets_test), # Validate on the test set\n",
    "        epochs=50, # Train for 50 epochs\n",
    "        batch_size=32 # Use a batch size of 32\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
